'''
/*
 * @Author: Limin Yang (liminy2@illinois.edu)
 * @Date: 2021-08-29 00:44:22
 * @Last Modified by: Limin Yang
 * @Last Modified time: 2021-08-29 03:49:06
 */

NOTE: get data for Fig. 2, 3, and 4.
    * ember-2018 does not provide reliable family labels, so consider it has no family info.

'''

import os

os.environ['PYTHONHASHSEED'] = '0'
from numpy.random import seed
import random
random.seed(1)
seed(1)

import sys
import logging
import traceback

import numpy as np
import pandas as pd #G6
import matplotlib.pylab as plt
from collections import Counter
from pprint import pformat
from timeit import default_timer as timer
from sklearn.model_selection import train_test_split

import bodmas.multiple_data as multiple_data
import bodmas.multiple_evaluate as evaluate
import bodmas.utils as utils
import bodmas.classifier as classifier
from bodmas.config import config
from bodmas.logger import init_log

import bodmas.g6helper as g6


MAX_TOP_N_ACC = 1
RUN_IDENTIFIER = "base"

def main():
    # ----------------------------------------------- #
    # 0. Init log path and parse args                 #
    # ----------------------------------------------- #

    args = utils.parse_multiple_dataset_args()

    # the log file would be "./logs/bluehex_main.log" and "./logs/bluehex_main.log.wf" if no redirect
    log_path = './logs/bluehex_main'
    if args.quiet:
        init_log(log_path, level=logging.INFO)
    else:
        init_log(log_path, level=logging.DEBUG)
    logging.warning('Running with configuration:\n' + pformat(vars(args)))
    logging.getLogger('matplotlib.font_manager').disabled = True

    # ----------------------------------------------- #
    # 1. global setting and load data                 #
    # ----------------------------------------------- #
    setting = args.setting_name
    GENERAL_DATA_FOLDER = f'multiple_data'
    DATA_FOLDER = f'multiple_data/{setting}'
    MODELS_FOLDER = f'multiple_models/{setting}'
    FIG_FOLDER = f'multiple_fig/{setting}'
    REPORT_FOLDER = f'multiple_reports/{setting}'
    os.makedirs(DATA_FOLDER, exist_ok=True)
    os.makedirs(MODELS_FOLDER, exist_ok=True)
    os.makedirs(FIG_FOLDER, exist_ok=True)
    os.makedirs(REPORT_FOLDER, exist_ok=True)

    global RUN_IDENTIFIER #G6

    task = args.task
    train_set = args.training_set
    diversity = args.diversity # only set value as "no" for this argument
    test_begin_time, test_end_time = args.testing_time.split(',')
    families_cnt = args.families
    SEED = args.seed

    POSTFIX, MODEL_POSTFIX = get_saved_file_postfix(task, diversity, test_begin_time, test_end_time,
                                                    train_set, families_cnt)

    # final feature vectors and labels for training and testing set
    SAVED_DATA_PATH = os.path.join(DATA_FOLDER, f'X_and_y_{POSTFIX}_r{SEED}.h5')


    X_train_origin, y_train_origin, X_test_list, y_test_list = \
        multiple_data.load_bluehex_data(task, test_begin_time, test_end_time, families_cnt,
                                        is_normalize=False,
                                        general_data_folder=GENERAL_DATA_FOLDER,
                                        setting_data_folder=DATA_FOLDER,
                                        saved_data_path=SAVED_DATA_PATH)


    logging.info('training and testing set prepared')
    X_train, X_val, y_train, y_val = train_test_split(X_train_origin, y_train_origin, test_size=0.2,
                                                      random_state=SEED, shuffle=True)

    NUM_FEATURES = X_train.shape[1]

    logging.info(f'after split, training: {X_train.shape}, validation: {X_val.shape}')
    logging.info(f'training label: {Counter(y_train)}')
    logging.info(f'validation label: {Counter(y_val)}')
    for idx, y_test in enumerate(y_test_list):
        logging.info(f'testing set {idx} label: {Counter(y_test)}')

    # G6 - Exports train data to an npy, stores seed # in the name
    save = True
    if save:
        file_path_for_Xsave = f'multiple_data/g6data/X_train_real_seed{SEED}.npy'
        file_path_for_Ysave = f'multiple_data/g6data/y_train_real_seed{SEED}.npy'
        np.save(file_path_for_Xsave, X_train)
        np.save(file_path_for_Ysave, y_train)
        logging.info(f'TRAINING DATA SAVED, seed: {SEED}')

    # G6 - Toggle accept mixed data set
    replace_training = not save
    if replace_training:
        # Load the synthetic data
        synth_X = np.load(f'multiple_data/g6data/synth_X_num_train.npy')
        synth_y = np.load(f'multiple_data/g6data/synth_y_train.npy')
        logging.info('Synthetic Data Loaded')

        # Make dataframes
        real_df = pd.DataFrame(X_train)
        real_df['y_label'] = y_train
        synth_df = pd.DataFrame(synth_X)
        synth_df['y_label'] = synth_y
        logging.info(f'CHANGING TRAINING DATA')

        # Create the synthethic data subset for augmentation
        synth_split = g6.sample_helper(real_df, synth_df)

        # Create the augmented dataset
        mixed = pd.concat([real_df, synth_split], ignore_index=True)
        y_train = mixed["y_label"].to_numpy()
        mixed.drop("y_label", axis=1, inplace=True)
        X_train = mixed.to_numpy()

        logging.info(f'Synthetic Data, initial shape: {synth_df.shape}, final shape: {synth_split.shape}')
        logging.info(f'Real Data, shape: {real_df.shape}')
        
        # Change the run identifier to change the file name of how predictions are saved
        RUN_IDENTIFIER = f'augmented'
        logging.info(f'Mixed Data, shape: {X_train.shape}, identifier: {RUN_IDENTIFIER}')
        logging.info(f'TRAINING DATA CHANGED')
    
    g6_filter = False # Toggles Test filtering later on
    if (g6_filter):
        g6_filter_min_pop = 0
        RUN_IDENTIFIER = f'{RUN_IDENTIFIER}_filter{g6_filter_min_pop}'
        logging.info(f'FILTER APPLIED, min_pop: {g6_filter_min_pop}')

    # ----------------------------------------------- #
    # 2. Train the classifier                         #
    # ----------------------------------------------- #

    clf = args.classifier
    retrain = args.retrain

    if clf == 'gbdt':
        SAVED_MODEL_PATH = os.path.join(MODELS_FOLDER, f'gbdt{MODEL_POSTFIX}_r{SEED}.txt')
        gbdt_clf = classifier.GBDTClassifier(saved_model_path=SAVED_MODEL_PATH)
        model = gbdt_clf.train(X_train, y_train, task, families_cnt, retrain, config['gbdt_params'])
    elif clf == 'mlp':
        # NOTE: multi-class MLP not implemented
        dims = utils.get_model_dims('MLP', NUM_FEATURES, args.mlp_hidden, 1)
        dims_str = '-'.join(map(str, dims))
        lr = args.mlp_lr
        batch = args.mlp_batch_size
        epochs = args.mlp_epochs
        dropout = args.mlp_dropout
        SAVED_MODEL_PATH = os.path.join(MODELS_FOLDER, f'mlp_{dims_str}_lr{lr}_b{batch}_e{epochs}_d{dropout}{MODEL_POSTFIX}.h5')
        mlp_clf = classifier.MLPClassifier(SAVED_MODEL_PATH, dims, dropout, verbose=1)
        model = mlp_clf.train(X_train, y_train, X_val, y_val, retrain, lr, batch, epochs)
    else: # clf == 'rf':
        tree = args.tree
        SAVED_MODEL_PATH = os.path.join(MODELS_FOLDER, f'rf_tree{tree}{MODEL_POSTFIX}.pkl')
        rf_clf = classifier.RFClassifier(SAVED_MODEL_PATH, tree)
        model = rf_clf.train(X_train, y_train, retrain)

    # ----------------------------------------------- #
    # 3. Evaluate the classifier                      #
    # ----------------------------------------------- #

    pred_begin = timer()
    REPORT_PATH = os.path.join(REPORT_FOLDER, f'{clf}_{task}_report_{POSTFIX}_r{SEED}.csv')

    if task == 'binary':
        if clf != 'rf':
            fpr_list_all, tpr_list_all, f1_list_all = [], [], [] # each has two lists (fpr threshold 0.01 and 0.001)
            for fpr_target_on_val in [0.001, 0.0001]:
                threshold, fpr, tpr, f1 = evaluate.evaluate_prediction_on_validation(model, X_val, y_val,
                                                                                     fpr_target_on_val,
                                                                                     model_name=clf)

                fpr_list = [fpr]
                tpr_list = [tpr]
                f1_list = [f1]

                logging.critical(f'validation set threshold: {threshold}')

                for idx, X_test, y_test in zip(range(len(X_test_list)), X_test_list, y_test_list):
                    phase = f'test_{idx}'
                    ALL_CLASSIFICATION_RESULT_PATH = os.path.join(REPORT_FOLDER, 'intermediate',
                                                                f'{clf}_test_{idx}_fpr{fpr_target_on_val}_all_classification_result_{POSTFIX}.csv')
                    utils.create_parent_folder(ALL_CLASSIFICATION_RESULT_PATH)
                    MISCLASSIFIED_RESULT_PATH = os.path.join(REPORT_FOLDER, 'intermediate',
                                                                  f'misclassified_{clf}_test_{idx}_fpr{fpr_target_on_val}_result.csv')
                    fpr, tpr, f1 = evaluate.evaluate_prediction_on_testing(model, phase, X_test, y_test, threshold,
                                                                           test_begin_time, test_end_time, SEED,
                                                                           ALL_CLASSIFICATION_RESULT_PATH,
                                                                           MISCLASSIFIED_RESULT_PATH,
                                                                           model_name=clf)
                    fpr_list.append(fpr)
                    tpr_list.append(tpr)
                    f1_list.append(f1)

                fpr_list_all.append(fpr_list)
                tpr_list_all.append(tpr_list)
                f1_list_all.append(f1_list)

            # add roc_auc_score for a fair comparison
            auc_score_list = evaluate.evaluate_auc_score(model, clf, X_val, y_val, X_test_list, y_test_list)
            evaluate.write_result_to_report(fpr_list_all, tpr_list_all, f1_list_all, auc_score_list, REPORT_PATH, is_rf=False)
        else:
            '''do not use FPR threshold for Random Forest classifier because it's a majority vote, threshold seems meaningless.'''
            evaluate.evaluate_rf_model_performance(model, X_val, y_val, X_test_list, y_test_list, REPORT_PATH)
    else:
        '''multi-class classification'''
        label_mapping_file = os.path.join(DATA_FOLDER, f'top_{families_cnt}_label_mapping.json')
        mapping = utils.load_json(label_mapping_file)

        acc_final_list = []
        inclass_acc_final_list = []
        for top_n_acc in range(1, MAX_TOP_N_ACC+1):
            acc_list = []
            inclass_acc_list = []
            '''top 2 acc means if one of two predicted labels with the biggest probabilities match
            with the ground-truth label, then we consider the prediction as correct.'''

            '''validation set'''
            acc, inclass_acc = multiclass_prediction_helper(clf, model, X_val, y_val, families_cnt,
                                                            top_n_acc, mapping, REPORT_FOLDER, phase='val')
            
            logging.critical(f'top_n_acc: {top_n_acc}, validation: acc {acc:.4f}, inclass_acc: {inclass_acc:.4f}')
            acc_list.append(acc)
            inclass_acc_list.append(inclass_acc)
            if (top_n_acc == 1):
                os.makedirs(f'multiple_data/g6data/predictions/pred_{RUN_IDENTIFIER}', exist_ok=True)

            '''testing sets'''
            for idx, X_test, y_test in zip(range(len(X_test_list)), X_test_list, y_test_list):

                # G6 - Filter low population test families
                if (g6_filter):
                    logging.info(f'Filtering with minimum family population: {g6_filter_min_pop}')
                    test_df = pd.DataFrame(X_test)
                    test_df['y_label'] = y_test
                    counts = test_df["y_label"].value_counts()
                    filtered = test_df[test_df["y_label"].isin(counts[counts >= g6_filter_min_pop].index)]
                    logging.info(f'Families after filtering: {len(filtered["y_label"].value_counts())}')
                    y_test = filtered["y_label"].to_numpy()
                    filtered.drop("y_label", axis=1, inplace=True)
                    X_test = filtered.to_numpy()
                    logging.info(f'Shapes, X_test: {X_test.shape}, y_test: {y_test.shape}')
                # G6 - End

                acc, inclass_acc = multiclass_prediction_helper(clf, model, X_test, y_test, families_cnt,
                                                                top_n_acc, mapping, REPORT_FOLDER, phase=f'test_{idx}')

                logging.critical(f'top_n_acc: {top_n_acc}, test-{idx}: acc {acc:.4f}, inclass_acc: {inclass_acc:.4f}')
                acc_list.append(acc)
                inclass_acc_list.append(inclass_acc)

            acc_final_list.append(acc_list)
            inclass_acc_final_list.append(inclass_acc_list)

        acc_final_list = np.transpose(np.array(acc_final_list)) # row: phase, column: topacc and inclass_acc
        inclass_acc_final_list = np.transpose(np.array(inclass_acc_final_list))
        evaluate.write_multiclass_result_to_report(acc_final_list, inclass_acc_final_list, REPORT_PATH)

    pred_end = timer()
    logging.info(f'prediction on validation and testing time: {pred_end - pred_begin:.1f} seconds')


def multiclass_prediction_helper(clf, model, X, y, families_cnt, top_n_acc, mapping, report_folder, phase):

    if clf == 'rf':
        # RF supports multi-class classification internally, so need to use predict_proba() to get the probabilities
        y_pred = model.predict_proba(X)
    else:
        y_pred = model.predict(X)

    unseen_family = families_cnt

    ''' use -1 to sort in descending order.
        another solution is to use np.flip(y_pred, axis=1)
        y_pred[::-1] would reverse with axis=0
    '''
    y_pred = np.argsort(-1 * y_pred, axis=1)[:, :top_n_acc]
    logging.debug(f'y_{phase}_pred shape: {y_pred.shape}')

    FIG_SAVE_FOLDER = os.path.join(report_folder, 'intermediate')
    utils.create_folder(FIG_SAVE_FOLDER)
    VAL_CM_FIG_PATH = os.path.join(FIG_SAVE_FOLDER, f'{clf}_family_{families_cnt}_topacc_{top_n_acc}_{phase}_confusion_matrix.png')

    acc, inclass_acc = evaluate.evaluate_multiclass_prediction(y, y_pred, unseen_family, top_n_acc,
                                                                VAL_CM_FIG_PATH, mapping, phase=phase)

    # G6 - Save y_pred into a dataframe with an identifying column
    if (top_n_acc == 1 and phase != 'val'):
        y_true = y.reshape(-1, 1)
        pred_df = pd.DataFrame(np.hstack((y_pred, y_true)), columns=['y_pred', 'y_true'])
        np.save(f'multiple_data/g6data/predictions/pred_{RUN_IDENTIFIER}/{phase}.npy', pred_df)
        logging.info(f'Predictions saved for {phase}, Prediction shape: {pred_df.shape}')
    # G6 - End
    return acc, inclass_acc


def get_saved_file_postfix(task, diversity, test_begin_time, test_end_time, train_set, families_cnt):
    if task == 'binary':
        if diversity == 'no':
            POSTFIX = f'test_{test_begin_time}_{test_end_time}'
            MODEL_POSTFIX = ''
    else:
        POSTFIX = f'{train_set}_families_{families_cnt}_test_{test_begin_time}_{test_end_time}'
        MODEL_POSTFIX = f'_{train_set}_families_{families_cnt}'

    return POSTFIX, MODEL_POSTFIX


if __name__ == "__main__":
    start = timer()
    main()
    end = timer()
    logging.info(f'time elapsed: {end - start:.2f}')
